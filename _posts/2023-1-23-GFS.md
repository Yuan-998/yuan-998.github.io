---
layout: post
title: Google File System
subtitle: Some questions regarding GFS
tags: [Distributed System]
readtime: true
---

## GFS Architecture
![Architecture](../assets/img/GFS/architecture.png)

### Meatadata in Master
1. `namespace` and `file names`
2. `file name` -> `array of chunk handles`
3. `chunk handles` -> `(version, list of chunkservers, primary, lease)`

### Read Operation
#### Workflow
1. Client maps `Filename + Offset` to `Filename + Chunk Index` and send request to master node.
   ##### Questions
      - Each file is stored sequentially? `Offset / Chunk Size == Chunk Index`?
         Write-append only.
      - Read request across multiple chunks.
         GFS library will notice this and seperate the request into two. Client reads data from two data nodes.
      - Each file begins with a new chunk? A chunk won't be shared with others?
         A chunk won't be shared by multiple files, which will bring the internal fragmentation problem. GFS adopts [lazy space allocation](#what-is-internal-fragmentation-why-does-lazy-allocation-help) to reduce this problem.
2. Master looks for `chunk handle + chunk locations` in metadata based on the `chunk index` and return them to client
   `Master holds the mapping of file name to array of chunk handles`
3. Clients caches the returned data, using `file name + chunk index` as key
4. Clients chooses the closest data node and get data with `chunk handle + chunk locations`

### Lease
To avoid that the master becomes the performance bottleneck. Distributed write requests to `Primary` Node.
Leader apporach to have total order.

#### No Read Lease in GFS?
Concurrent reads will never be a problem. The possible problem is that there is writing and reading at the same time. ~~But remember that GFS is `record-append only`. There is no modification on the old data. New data can only be appended at the end of files, whichi means there is will never be the case that a read and a write being applied on the same data.~~

> We support the usual operations to *create*, *delete*, *open*, *close*, *read*, and *write* files.
> Moreover, GFS has *snapshot* and *record append* operations. [...] Record append allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity of each individual client's append.


### Write Operation
![write](../assets/img/GFS/write.png)
1. Client ask master which chunkserver is `Primary` and which chunkservers are `Secondary`. Master grants a lease to some chunkserver when there is no `Primary`
2. Client caches the information from master and only contacts master further when `Primary` is unreachable or lease expires (lease information is in metadata)
3. Clients sends append log (data) to every chunkserver. Chunkservers write the request to LRU **caches** (not on disk) first.
4. On receiving acknowledge from all chunkservers, client sends append request (operation) to `Primary`. `Primary` serializes requests (when mulitple request exist) and write them locally first.
5. `Primary` sends requests and the order of requests to all `Secondary`.
6. `Secondary` responds to `Primary` after finishing handling the requests.
7. Start over `step 3-7` on any failure from `Secondary`.

### Consistency Model
GFS is a weak consistency model. Not every copy of data are identical.
![consistency](../assets/img/GFS/consistency.png)

`defined`: After a file data mutation, a file region is consistent and clients will see what the mutation writes in its entirety.
`consistent`: All clients will always see the same data, regardless of which replicas they read from.

For `write`: write might succeed on some replicas and might fail on some replicas, which results in data inconsistency between replicas.

For `record append`: retry on failure. But the retry will not be carried out at the previous offset but the previous offset plus failed part. This will cause permanent inconsistency and duplicate data.

- serial success write: defined
- concurrent write: consistent but undefined. 
  >If a write by the application is large or straddles a chunk boundary, GFS client code breaks it down into multiple write operations. They [...] may be interleaved with and overwritten by concurrent operations from other clients. Therefore, the shared file region may end up containing fragments from different clients, although the replicas will be identical because the individual operations are completed successfully in the same order on all replicas. This leaves the file region in consistent but undefined state [...].
- serial and concurrent success record append: defined with inconsistent data region


### Some Questions from MIT 6.824
Refering from [here](http://nil.csail.mit.edu/6.824/2021/papers/gfs-faq.txt).
##### How does an application know what sections of a chunk consist of padding and duplicate records?
To detect padding, applications can put a **predictable magic number** at the **start of a valid record**, or include a **checksum** that will likely only be valid if the record is valid. The application can detect duplicates by including unique IDs in records. Then, if it reads a record that has the same ID as an earlier record, it knows that they are duplicates of each other. GFS provides a library for applications that handles these cases.

##### How can clients find their data given that atomic record append writes it at an unpredictable offset in the file?
Append (and GFS in general) is mostly intended for applications that sequentially read entire files. Such applications will scan the file looking for valid records (see the previous question), so they don't need to know the record locations in advance. For example, the file might contain the set of link URLs encountered by a set of concurrent web crawlers. The file offset of any given URL doesn't matter much; readers just want to be able to read the entire set of URLs.

##### 64 megabytes sounds awkwardly large for the chunk size!
The 64 MB chunk size is the unit of book-keeping in the master, and the granularity at which files are sharded over chunkservers. Clients could issue smaller reads and writes -- they were not forced to deal in whole 64 MB chunks. The point of using such a big chunk size is to reduce the size of the meta-data tables in the master, and to avoid limiting clients that want to do huge transfers to reduce overhead. On the other hand, files less than 64 MB in size do not get much parallelism.

##### What if the master fails?
There are replica masters with a full copy of the master state; the paper's design requires human intervention to switch to one of the replicas after a master failure (Section 5.1.3). We will see later how to build replicated services with automatic cut-over to a backup, using Raft.

##### What is internal fragmentation? Why does lazy allocation help?
Internal fragmentation is the space wasted when a system uses an allocation unit larger than needed for the requested allocation. For example, in GFS the risk is an application creates a 1-byte file and 64M-1byte is wasted, because the allocation size is 64MB (a chunk). GFS avoids this potential problem, because the 64MB is lazy allocated. Every chunk is a Linux file, and so when an application creates 1-byte file, the on-disk representation of the chunk is 1-byte Linux file.